# Module 2: BYO Retrieval + Memory

## Complexity: üî¥ Complex
Break into 5 sub-plans for manageable execution.

---

## Overview

Replace OpenAI's managed RAG with our own retrieval system. Switch from Responses API to Chat Completions API for provider flexibility. Build document ingestion pipeline with chunking, embedding, and pgvector storage.

**Key Changes:**
- Remove OpenAI thread management (we handle conversation history)
- Remove openai_thread_id from schema
- Add documents, chunks tables with vector embeddings
- Add file upload + processing pipeline
- Use any OpenAI-compatible API (OpenRouter, Ollama, LM Studio)

---

## Sub-Plan Breakdown

| Sub-Plan | Name | Complexity |
|----------|------|------------|
| 2.1 | Schema Refactor + Chat Completions Migration | ‚ö†Ô∏è Medium |
| 2.2 | Document Storage + Upload UI | ‚ö†Ô∏è Medium |
| 2.3 | Chunking + Embedding Pipeline | ‚ö†Ô∏è Medium |
| 2.4 | Retrieval Tool + RAG Integration | ‚ö†Ô∏è Medium |
| 2.5 | Realtime Ingestion Status | ‚úÖ Simple |

---

## New Database Schema

```sql
-- Keep threads table but remove openai_thread_id
ALTER TABLE threads DROP COLUMN openai_thread_id;

-- Documents
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
    filename TEXT NOT NULL,
    file_type TEXT NOT NULL,
    file_size INTEGER NOT NULL,
    storage_path TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending' CHECK (status IN ('pending', 'processing', 'completed', 'failed')),
    error_message TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Chunks (with embeddings)
CREATE TABLE chunks (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
    content TEXT NOT NULL,
    chunk_index INTEGER NOT NULL,
    embedding vector(1536),  -- OpenAI ada-002 dimension
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_documents_user_id ON documents(user_id);
CREATE INDEX idx_documents_status ON documents(status);
CREATE INDEX idx_chunks_document_id ON chunks(document_id);
CREATE INDEX idx_chunks_embedding ON chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

-- RLS
ALTER TABLE documents ENABLE ROW LEVEL SECURITY;
ALTER TABLE chunks ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can CRUD own documents" ON documents
    FOR ALL USING (auth.uid() = user_id);

CREATE POLICY "Users can read chunks from own documents" ON chunks
    FOR ALL USING (EXISTS (
        SELECT 1 FROM documents WHERE documents.id = chunks.document_id
        AND documents.user_id = auth.uid()
    ));
```

---

## API Endpoints

```
# Existing (modified)
POST   /api/threads              # Create thread (no OpenAI thread)
GET    /api/threads              # List user's threads
GET    /api/threads/{id}         # Get thread + messages
DELETE /api/threads/{id}         # Delete thread
POST   /api/threads/{id}/messages # Send message (Chat Completions + RAG)

# New - Documents
POST   /api/documents            # Upload file
GET    /api/documents            # List user's documents
GET    /api/documents/{id}       # Get document details
DELETE /api/documents/{id}       # Delete document + chunks
```

---

## Environment Variables

**New/Updated:**
```
# Replace OPENAI_API_KEY with generic config
LLM_API_BASE=https://openrouter.ai/api/v1  # Or http://localhost:1234/v1 for LM Studio
LLM_API_KEY=your-key
LLM_MODEL=openai/gpt-4o  # OpenRouter model ID

# Embedding model (keep OpenAI for embeddings or use local)
EMBEDDING_API_BASE=https://api.openai.com/v1
EMBEDDING_API_KEY=your-openai-key
EMBEDDING_MODEL=text-embedding-ada-002
```

---

## Sub-Plan 2.1: Schema Refactor + Chat Completions Migration

### Tasks
1. Create migration `002_byo_retrieval.sql`:
   - Drop `openai_thread_id` from threads
   - Create documents table
   - Create chunks table with vector column
   - Add RLS policies
2. Update `openai_service.py` ‚Üí `llm_service.py`:
   - Remove thread management (create_thread, add_message)
   - Implement `chat_completion_stream()` using generic OpenAI-compatible endpoint
   - Load full message history from DB, send to LLM
3. Update `supabase_service.py`:
   - Remove `openai_thread_id` from thread creation
   - Add methods for documents/chunks CRUD
4. Update `chat.py` routes:
   - Remove OpenAI thread creation
   - Build message history from DB for each request
5. Update frontend `useChat.ts`:
   - Remove any OpenAI thread references

### Validation
- Create thread works (no OpenAI thread created)
- Send message works with Chat Completions API
- Message history persists and is sent correctly
- Switching LLM_API_BASE changes provider

---

## Sub-Plan 2.2: Document Storage + Upload UI

### Tasks
1. Configure Supabase Storage bucket `documents` with RLS
2. Create `storage_service.py`:
   - `upload_file()` - upload to Supabase Storage
   - `delete_file()` - remove from storage
3. Create document API routes in `documents.py`:
   - POST /api/documents - upload + create record
   - GET /api/documents - list user's documents
   - DELETE /api/documents/{id} - delete document + storage
4. Create frontend components:
   - `DocumentList.tsx` - show uploaded documents with status
   - `DocumentUpload.tsx` - drag-and-drop upload zone
   - `useDocuments.ts` hook
5. Add Documents page/tab to UI

### Validation
- Can upload a text file via drag-and-drop
- File appears in Supabase Storage
- Document record created with status 'pending'
- Can delete document (removes storage + record)
- RLS enforced (users only see own documents)

---

## Sub-Plan 2.3: Chunking + Embedding Pipeline

### Tasks
1. Create `chunking_service.py`:
   - `chunk_text()` - split text into chunks (recursive character splitter)
   - Configurable chunk size/overlap via env vars
2. Create `embedding_service.py`:
   - `embed_text()` - get embedding from API
   - `embed_chunks()` - batch embed multiple chunks
3. Create `ingestion_service.py`:
   - `process_document()` - orchestrate: read file ‚Üí chunk ‚Üí embed ‚Üí store
4. Add background processing (triggered after upload):
   - Update document status: pending ‚Üí processing ‚Üí completed/failed
5. Store chunks with embeddings in pgvector

### Validation
- Upload triggers processing automatically
- Document status updates correctly
- Chunks created with correct embeddings (1536 dimensions)
- Error handling: failed status + error_message on failure

---

## Sub-Plan 2.4: Retrieval Tool + RAG Integration

### Tasks
1. Create `retrieval_service.py`:
   - `search_chunks()` - vector similarity search
   - Returns top-k chunks with scores
   - Filter by user_id (RLS)
2. Update `llm_service.py`:
   - Add retrieval tool definition
   - Handle tool calls in streaming response
   - Inject retrieved context into messages
3. Update chat flow:
   - On message: retrieve relevant chunks ‚Üí add to system prompt ‚Üí stream response
4. Display retrieval in UI (optional):
   - Show which documents were used

### Validation
- Chat retrieves relevant chunks from user's documents
- Retrieved context improves answers
- No cross-user data leakage
- Works with different LLM providers

---

## Sub-Plan 2.5: Realtime Ingestion Status

### Tasks
1. Configure Supabase Realtime on documents table
2. Create `useDocumentStatus.ts` hook:
   - Subscribe to document status changes
   - Update UI in realtime
3. Update DocumentList to show live status updates
4. Add visual indicators: spinner for processing, checkmark for done, X for failed

### Validation
- Upload a document, see status change from pending ‚Üí processing ‚Üí completed
- No page refresh needed
- Multiple documents process correctly

---

## Verification Checklist

- [ ] Threads work without OpenAI thread management
- [ ] Chat works with Chat Completions API (not Responses API)
- [ ] Can switch LLM provider via env vars
- [ ] Document upload + storage works
- [ ] Chunking produces reasonable chunks
- [ ] Embeddings stored in pgvector
- [ ] Vector search returns relevant results
- [ ] Chat uses retrieved context
- [ ] Realtime status updates work
- [ ] RLS enforced on documents/chunks

---

## Files to Create/Modify

### New Files
1. `supabase/migrations/002_byo_retrieval.sql`
2. `backend/app/services/llm_service.py`
3. `backend/app/services/storage_service.py`
4. `backend/app/services/chunking_service.py`
5. `backend/app/services/embedding_service.py`
6. `backend/app/services/ingestion_service.py`
7. `backend/app/services/retrieval_service.py`
8. `backend/app/api/routes/documents.py`
9. `backend/app/models/document.py`
10. `frontend/src/pages/Documents.tsx`
11. `frontend/src/components/documents/DocumentList.tsx`
12. `frontend/src/components/documents/DocumentUpload.tsx`
13. `frontend/src/hooks/useDocuments.ts`
14. `frontend/src/hooks/useDocumentStatus.ts`

### Modified Files
1. `backend/app/config.py` - new env vars
2. `backend/app/main.py` - add documents router
3. `backend/app/services/supabase_service.py` - documents/chunks methods
4. `backend/app/services/openai_service.py` - remove (replaced by llm_service)
5. `backend/app/api/routes/chat.py` - use llm_service, build history
6. `backend/app/models/chat.py` - remove openai_thread_id
7. `frontend/src/App.tsx` - add Documents route
8. `frontend/src/hooks/useChat.ts` - remove OpenAI thread refs
9. `frontend/src/components/layout/Sidebar.tsx` - add Documents nav
